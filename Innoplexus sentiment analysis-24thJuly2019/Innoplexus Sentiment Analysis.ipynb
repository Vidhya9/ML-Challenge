{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  \n",
    "import nltk\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\",200)\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0</td>\n",
       "      <td>Autoimmune diseases tend to come in clusters. As for Gilenya – if you feel good, don’t think about it, it won’t change anything but waste your time and energy. I’m taking Tysabri and feel amazing,...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9eba8f80e7e20f3a2f48685530748fbfa95943e4</td>\n",
       "      <td>I can completely understand why you’d want to try it. But, results reported in lectures don’t always stand up to the scrutiny of peer-review during publication. There so much still to do before th...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fe809672251f6bd0d986e00380f48d047c7e7b76</td>\n",
       "      <td>Interesting that it only targets S1P-1/5 receptors rather than 1-5 like Fingolimod. Hoping to soon see what the AEs and SAEs were Yes. I'm not sure what this means, exactly:  Quote Nine patients r...</td>\n",
       "      <td>fingolimod</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bd22104dfa9ec80db4099523e03fae7a52735eb6</td>\n",
       "      <td>Very interesting, grand merci. Now I wonder where lemtrada and ocrevus sales would go, if they prove anti-cd20 are induction</td>\n",
       "      <td>ocrevus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b227688381f9b25e5b65109dd00f7f895e838249</td>\n",
       "      <td>Hi everybody, My latest MRI results for Brain and Cervical Cord are in and my next Neurologist appointment is in the next couple of weeks. There’re no new lesions in Brain/Cord and I’ve had no rel...</td>\n",
       "      <td>gilenya</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  2e180be4c9214c1f5ab51fd8cc32bc80c9f612e0   \n",
       "1  9eba8f80e7e20f3a2f48685530748fbfa95943e4   \n",
       "2  fe809672251f6bd0d986e00380f48d047c7e7b76   \n",
       "3  bd22104dfa9ec80db4099523e03fae7a52735eb6   \n",
       "4  b227688381f9b25e5b65109dd00f7f895e838249   \n",
       "\n",
       "                                                                                                                                                                                                      text  \\\n",
       "0  Autoimmune diseases tend to come in clusters. As for Gilenya – if you feel good, don’t think about it, it won’t change anything but waste your time and energy. I’m taking Tysabri and feel amazing,...   \n",
       "1  I can completely understand why you’d want to try it. But, results reported in lectures don’t always stand up to the scrutiny of peer-review during publication. There so much still to do before th...   \n",
       "2  Interesting that it only targets S1P-1/5 receptors rather than 1-5 like Fingolimod. Hoping to soon see what the AEs and SAEs were Yes. I'm not sure what this means, exactly:  Quote Nine patients r...   \n",
       "3                                                                             Very interesting, grand merci. Now I wonder where lemtrada and ocrevus sales would go, if they prove anti-cd20 are induction   \n",
       "4  Hi everybody, My latest MRI results for Brain and Cervical Cord are in and my next Neurologist appointment is in the next couple of weeks. There’re no new lesions in Brain/Cord and I’ve had no rel...   \n",
       "\n",
       "         drug  sentiment  \n",
       "0     gilenya          2  \n",
       "1     gilenya          2  \n",
       "2  fingolimod          2  \n",
       "3     ocrevus          2  \n",
       "4     gilenya          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train_F3WbcTw.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9e9a8166b84114aca147bf409f6f956635034c08</td>\n",
       "      <td>256 (previously stable on natalizumab), with 55% switching to fingolimod</td>\n",
       "      <td>fingolimod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>e747e6822c867571afe7b907b51f0f2ca67b0e1a</td>\n",
       "      <td>On fingolimod and have been since December 2015; the only way I can describe it any better is like feeling drunk but without having had a drink!!!</td>\n",
       "      <td>fingolimod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50b6d851bcff4f35afe354937949e9948975adf7</td>\n",
       "      <td>Apparently it's shingles! :-/ I do have a few red spots just below my left breast which appeared a few days ago. Wasn't sure if they were connected or not, but showed them to the GP and he immedia...</td>\n",
       "      <td>humira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae</td>\n",
       "      <td>If the Docetaxel doing once a week x3 weeks then 1 week off (claim to be less harsh), will it have the same efficacy as the once every 3 weeks?   Diagnosed in 2014 stage 4 nsclc. EGFR+ (also posit...</td>\n",
       "      <td>tagrisso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8b37d169dee5bdae27060949242fb54feb6a7f7f</td>\n",
       "      <td>CC, Stelara worked in a matter of days for me. If your GI is willing to jump through hoops to get it for you then it’s probably worth considering. If one anti tnf stopped working for you then anot...</td>\n",
       "      <td>stelara</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  9e9a8166b84114aca147bf409f6f956635034c08   \n",
       "1  e747e6822c867571afe7b907b51f0f2ca67b0e1a   \n",
       "2  50b6d851bcff4f35afe354937949e9948975adf7   \n",
       "3  7f82ec2176ae6ab0b5d20b5ffc767ac829f384ae   \n",
       "4  8b37d169dee5bdae27060949242fb54feb6a7f7f   \n",
       "\n",
       "                                                                                                                                                                                                      text  \\\n",
       "0                                                                                                                                 256 (previously stable on natalizumab), with 55% switching to fingolimod   \n",
       "1                                                       On fingolimod and have been since December 2015; the only way I can describe it any better is like feeling drunk but without having had a drink!!!   \n",
       "2  Apparently it's shingles! :-/ I do have a few red spots just below my left breast which appeared a few days ago. Wasn't sure if they were connected or not, but showed them to the GP and he immedia...   \n",
       "3  If the Docetaxel doing once a week x3 weeks then 1 week off (claim to be less harsh), will it have the same efficacy as the once every 3 weeks?   Diagnosed in 2014 stage 4 nsclc. EGFR+ (also posit...   \n",
       "4  CC, Stelara worked in a matter of days for me. If your GI is willing to jump through hoops to get it for you then it’s probably worth considering. If one anti tnf stopped working for you then anot...   \n",
       "\n",
       "         drug  \n",
       "0  fingolimod  \n",
       "1  fingolimod  \n",
       "2      humira  \n",
       "3    tagrisso  \n",
       "4     stelara  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('test_tOlRoBf.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5279, 4)\n",
      "(2924, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    3825\n",
       "1     837\n",
       "0     617\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sentiment.value_counts()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced class problem.\n",
    "1. Random Under Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random under-sampling:\n",
      "1    837\n",
      "2    837\n",
      "0    617\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Class count\n",
    "count_class_2, count_class_1 ,count_class_0 = train.sentiment.value_counts()\n",
    "\n",
    "# Divide by class\n",
    "class_0 = train[train.sentiment == 0]\n",
    "class_1 =train[train.sentiment == 1]\n",
    "class_2 = train[train.sentiment == 2]\n",
    "class_2_under = class_2.sample(count_class_1)\n",
    "train_under = pd.concat([class_2_under, class_1,class_0], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "#print(df_test_under.sentiment.value_counts())\n",
    "print(train1.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Random Over Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_0_over = class_0.sample(count_class_2, replace=True)\n",
    "class_1_over = class_1.sample(count_class_2, replace=True)\n",
    "train_over = pd.concat([class_2, class_1_over , class_0_over], axis=0)\n",
    "\n",
    "print('Random under-sampling:')\n",
    "#print(df_test_under.sentiment.value_counts())\n",
    "print(train1.sentiment.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2291, 4)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_under.shape)\n",
    "print(train_over.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_hash    0\n",
       "text           0\n",
       "drug           0\n",
       "sentiment      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_new.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8203, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Undersampling is giving more accurate result\n",
    "combi = train.append(test , ignore_index=True)\n",
    "#combi = train_over.append(test , ignore_index=True)\n",
    "combi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gilenya                         697\n",
       "ocrevus                         629\n",
       "ocrelizumab                     362\n",
       "entyvio                         303\n",
       "humira                          289\n",
       "remicade                        276\n",
       "opdivo                          223\n",
       "tarceva                         215\n",
       "fingolimod                      202\n",
       "keytruda                        198\n",
       "tagrisso                        189\n",
       "cladribine                      174\n",
       "stelara                         163\n",
       "alimta                          140\n",
       "lucentis                         80\n",
       "eylea                            57\n",
       "avastin                          53\n",
       "cimzia                           50\n",
       "alectinib                        45\n",
       "erlotinib                        41\n",
       "nivolumab                        39\n",
       "simponi                          38\n",
       "crizotinib                       37\n",
       "vitrectomy                       36\n",
       "pembrolizumab                    35\n",
       "xalkori                          33\n",
       "mavenclad                        31\n",
       "tecentriq                        31\n",
       "tofacitinib                      30\n",
       "tysabri                          29\n",
       "                               ... \n",
       "alemtuzumab                       3\n",
       "arzerra                           3\n",
       "almita                            3\n",
       "zykadia                           2\n",
       "duvalumab                         2\n",
       "movectro                          2\n",
       "pf-00547659                       2\n",
       "stellara                          2\n",
       "risankizumab                      2\n",
       "gilotrif                          2\n",
       "brolucizumab                      2\n",
       "nivolumabb                        1\n",
       "cyramza                           1\n",
       "elotinib                          1\n",
       "pemetrexed disodium               1\n",
       "pan-retinal photocoagulation      1\n",
       "alunbrig                          1\n",
       "crizotnib                         1\n",
       "afainib                           1\n",
       "teriflunomide                     1\n",
       "flixabi                           1\n",
       "pegaptanib                        1\n",
       "alitma                            1\n",
       "portrazza                         1\n",
       "etrolizumab                       1\n",
       "pemrolizumab                      1\n",
       "ketruda                           1\n",
       "osmertinib                        1\n",
       "necitumumab                       1\n",
       "infliximab-dyyb                   1\n",
       "Name: drug, Length: 103, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi['drug'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unique_hash', 'text', 'drug', 'sentiment', 'tidy_text'], dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding Drug Column\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encode=LabelEncoder()\n",
    "combi.iloc[:,2]=encode.fit_transform(combi.iloc[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Text Column\n",
    "combi['tidy_text'] = combi['text'].str.replace(r\"http\\S+\", \"\")\n",
    "combi['tidy_text'] = combi['tidy_text'].str.replace(r\"http\", \"\")\n",
    "combi['tidy_text'] = combi['tidy_text'].str.replace(r\"(\\d)\", \"\")\n",
    "combi['tidy_text'] = combi['tidy_text'].str.replace(r\"@\\S+\", \"\")\n",
    "combi['tidy_text'] = combi['tidy_text'].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "combi['tidy_text'] = combi['tidy_text'].str.replace(r\"@\", \"at\")\n",
    "combi['tidy_text'] = combi['tidy_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_hash</th>\n",
       "      <th>text</th>\n",
       "      <th>drug</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tidy_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4d95c5ec996f8c2329865bf7e94aad11ec7d7c8c</td>\n",
       "      <td>Hello @mermaidia11 , I get that completely. I am not PPMS myself, but I really just believe that PPMS folk should at least have the choice, as they do in the US, Canada and Europe. I take Tecfider...</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>hello , get completely ppms myself, really believe ppms folk least choice, us, canada europe take tecfidera, complications, including deaths reported taking risk balance chosen take people died ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f6ccc04eadefc43e3079a471c7c5bce8a10cd944</td>\n",
       "      <td>I just read this entire thread. I am going to be starting on Entyvio as soon as insurance oks. This will be my first (hopefully last!) biologic. The reason I chose this is because it's not suppose...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>read entire thread going starting entyvio soon insurance oks first (hopefully last!) biologic reason chose supposed systemic hopefully make prone colds that's biggest worry biologic long term yo f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b72ecb1805f926ca9e8b5d92a1e81b96f43a5e84</td>\n",
       "      <td>Tuesday 03.28.17 - 8:33pm ET The FDA approved Genentech’s OCREVUS  TM (ocrelizumab) for people with relapsing or primary progressive forms of multiple sclerosis (MS). This is the first and only ap...</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>tuesday pm et fda approved genentech ocrevus tm (ocrelizumab) people relapsing primary progressive forms multiple sclerosis (ms) first approved treatment primary progressive ms (ppms) , one disabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5261dd6ea6b16be98d44ac5a51dc13fb299be3e8</td>\n",
       "      <td>Hi Mohit, Welcome to GRACE. I am sorry to hear of the recent progression of your father’s lung cancer. Though there are some oncologists who like to continue Tarceva even after progression, combin...</td>\n",
       "      <td>91</td>\n",
       "      <td>2</td>\n",
       "      <td>hi mohit, welcome grace sorry hear recent progression father lung cancer though oncologists like continue tarceva even progression, combining drug alimta (pemetrexed), often progression significan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>163409436fce5b7f662a743024fbdd0ae5a32f87</td>\n",
       "      <td>Comment: Diagnosed with wet ARMD about a year ago. Was switched to Eylea several months ago. It initially had a profound impact reducing the wet leakage but not entirely. Leakage returned when a b...</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>comment diagnosed wet armd year ago switched eylea several months ago initially profound impact reducing wet leakage entirely leakage returned bi monthly injection skipped next injection restored ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                unique_hash  \\\n",
       "0  4d95c5ec996f8c2329865bf7e94aad11ec7d7c8c   \n",
       "1  f6ccc04eadefc43e3079a471c7c5bce8a10cd944   \n",
       "2  b72ecb1805f926ca9e8b5d92a1e81b96f43a5e84   \n",
       "3  5261dd6ea6b16be98d44ac5a51dc13fb299be3e8   \n",
       "4  163409436fce5b7f662a743024fbdd0ae5a32f87   \n",
       "\n",
       "                                                                                                                                                                                                      text  \\\n",
       "0  Hello @mermaidia11 , I get that completely. I am not PPMS myself, but I really just believe that PPMS folk should at least have the choice, as they do in the US, Canada and Europe. I take Tecfider...   \n",
       "1  I just read this entire thread. I am going to be starting on Entyvio as soon as insurance oks. This will be my first (hopefully last!) biologic. The reason I chose this is because it's not suppose...   \n",
       "2  Tuesday 03.28.17 - 8:33pm ET The FDA approved Genentech’s OCREVUS  TM (ocrelizumab) for people with relapsing or primary progressive forms of multiple sclerosis (MS). This is the first and only ap...   \n",
       "3  Hi Mohit, Welcome to GRACE. I am sorry to hear of the recent progression of your father’s lung cancer. Though there are some oncologists who like to continue Tarceva even after progression, combin...   \n",
       "4  Comment: Diagnosed with wet ARMD about a year ago. Was switched to Eylea several months ago. It initially had a profound impact reducing the wet leakage but not entirely. Leakage returned when a b...   \n",
       "\n",
       "   drug  sentiment  \\\n",
       "0    65          2   \n",
       "1    32          2   \n",
       "2    65          2   \n",
       "3    91          2   \n",
       "4    35          2   \n",
       "\n",
       "                                                                                                                                                                                                 tidy_text  \n",
       "0  hello , get completely ppms myself, really believe ppms folk least choice, us, canada europe take tecfidera, complications, including deaths reported taking risk balance chosen take people died ta...  \n",
       "1  read entire thread going starting entyvio soon insurance oks first (hopefully last!) biologic reason chose supposed systemic hopefully make prone colds that's biggest worry biologic long term yo f...  \n",
       "2  tuesday pm et fda approved genentech ocrevus tm (ocrelizumab) people relapsing primary progressive forms multiple sclerosis (ms) first approved treatment primary progressive ms (ppms) , one disabl...  \n",
       "3  hi mohit, welcome grace sorry hear recent progression father lung cancer though oncologists like continue tarceva even progression, combining drug alimta (pemetrexed), often progression significan...  \n",
       "4  comment diagnosed wet armd year ago switched eylea several months ago initially profound impact reducing wet leakage entirely leakage returned bi monthly injection skipped next injection restored ...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "combi['tidy_text']=combi['tidy_text'].apply(lambda x : \" \".join([w for w in x.split() if w.lower() not in stop]))\n",
    "combi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [hello, ,, get, completely, ppms, myself,, really, believe, ppms, folk, least, choice,, us,, canada, europe, take, tecfidera,, complications,, including, deaths, reported, taking, risk, balance, c...\n",
       "1    [read, entire, thread, going, starting, entyvio, soon, insurance, oks, first, (hopefully, last!), biologic, reason, chose, supposed, systemic, hopefully, make, prone, colds, that's, biggest, worry...\n",
       "2    [tuesday, pm, et, fda, approved, genentech, ocrevus, tm, (ocrelizumab), people, relapsing, primary, progressive, forms, multiple, sclerosis, (ms), first, approved, treatment, primary, progressive,...\n",
       "3    [hi, mohit,, welcome, grace, sorry, hear, recent, progression, father, lung, cancer, though, oncologists, like, continue, tarceva, even, progression,, combining, drug, alimta, (pemetrexed),, often...\n",
       "4    [comment, diagnosed, wet, armd, year, ago, switched, eylea, several, months, ago, initially, profound, impact, reducing, wet, leakage, entirely, leakage, returned, bi, monthly, injection, skipped,...\n",
       "Name: tidy_text, dtype: object"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = combi['tidy_text'].apply(lambda x: x.split())\n",
    "tokenized_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem.porter import *\n",
    "stemmer=PorterStemmer()\n",
    "tokenized_text = tokenized_text.apply(lambda x: [stemmer.stem(i) for i in x])   #stemming\n",
    "#Lemmatizing - is giving less accurate so used\n",
    "#from nltk.stem import WordNetLemmatizer \n",
    "#lemmatizer = WordNetLemmatizer() \n",
    "#tokenized_text1 = tokenized_text.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])   #lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatizing is giving less accurate result so stemming is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stitch these tokens back together\n",
    "for i in range(len(tokenized_text)):\n",
    "    tokenized_text[i] = ' '.join(tokenized_text[i])\n",
    "combi['tidy_text'] = tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5215, 1000)\n",
      "(5215, 1001)\n"
     ]
    }
   ],
   "source": [
    "#Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_vectorizer=CountVectorizer(max_df=0.90,min_df=2,max_features=1000,stop_words='english')\n",
    "bow = bow_vectorizer.fit_transform(combi['tidy_text'])\n",
    "print(bow.shape)\n",
    "bow=pd.DataFrame(bow.toarray())\n",
    "bow['drug']=combi['drug']\n",
    "print(bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5215, 1000)\n",
      "(5215, 1001)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer=TfidfVectorizer(max_df=0.90,min_df=2,max_features=1000,stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(combi['tidy_text'])\n",
    "print(tfidf.shape)\n",
    "tfidf=pd.DataFrame(tfidf.toarray())\n",
    "tfidf['drug']=combi['drug']\n",
    "print(tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20080289, 21513240)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "tokenized_text = combi['tidy_text'].apply(lambda x: x.split()) # tokenizing \n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            tokenized_text,\n",
    "            size=200, # desired no. of features/independent variables-300\n",
    "            window=5, # context window size -7\n",
    "            min_count=2,  # Minimum word count threshold\n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 4, # no.of cores\n",
    "            #sampling=#0=1e-3      #Downsample setting for frequent words-  1e-5  \n",
    "            seed = 34) \n",
    "model_w2v.train(tokenized_text, total_examples= len(combi['tidy_text']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not in vocabulary                                     \n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5215, 200)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparing word2vec feature set…\n",
    "wordvec_arrays = np.zeros((len(tokenized_text), 200)) \n",
    "for i in range(len(tokenized_text)):\n",
    "    wordvec_arrays[i,:] = word_vector(tokenized_text[i], 200)\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays) \n",
    "print(wordvec_df.shape)   \n",
    "#Now we have 200 new features, whereas in Bag of Words and TF-IDF we had 1000 features.\n",
    "wordvec_df['drug']=combi['drug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "tqdm.pandas(desc=\"progress-bar\") \n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(twt):\n",
    "    output = []\n",
    "    for i, s in zip(twt.index, twt):\n",
    "        output.append(LabeledSentence(s, [\"text_\" + str(i)]))\n",
    "    return output\n",
    "labeled_texts = add_label(tokenized_text) # label all the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledSentence(words=['hello', ',', 'get', 'complet', 'ppm', 'myself,', 'realli', 'believ', 'ppm', 'folk', 'least', 'choice,', 'us,', 'canada', 'europ', 'take', 'tecfidera,', 'complications,', 'includ', 'death', 'report', 'take', 'risk', 'balanc', 'chosen', 'take', 'peopl', 'die', 'take', 'paracetamol', 'drug', 'risk', 'get', 'drain', 'nh', 'point', 'ocrevu', 'consult', 'document,', 'said', 'ocrevu', 'favour', 'compar', 'cost', 'longer', 'term', 'ppm', 'patient', 'make', 'claim,', 'would', 'let', 'us', 'know', 'project', 'cost', 'lifelong', 'care', 'ppm', 'patient', 'without', 'ocrevus,', 'possibl', 'rrms,', 'long', 'list', 'medications,', 'includ', 'ocrevu', 'elig', 'sister', 'younger', 'me,', 'young', 'kids,', 'ppm', 'unfair', 'access', 'dmd', 'nh', 'prepar', 'fund', 'ocrevus,', 'littl', 'sister', 'ethic', 'standard', 'like', 'say', 'worth', 'invest', 'hey', 'ho,', 'least,', 'latest', 'develop', 'open', 'intellig', 'debate,', 'differ', 'opinion', 'keep', 'site', 'balanc', 'inform', 'respect', 'poster', 'reader', 'x'], tags=['text_0']),\n",
       " LabeledSentence(words=['read', 'entir', 'thread', 'go', 'start', 'entyvio', 'soon', 'insur', 'ok', 'first', '(hope', 'last!)', 'biolog', 'reason', 'chose', 'suppos', 'system', 'hope', 'make', 'prone', 'cold', \"that'\", 'biggest', 'worri', 'biolog', 'long', 'term', 'yo', 'femal', 'diagnos', 'w', 'uc', 'mid', 'transvers', ',', 'uc', \"crohn'\", 'coliti', 'current', 'med', 'lialda', 'tab', 'x', 'day,', 'uceri', 'x,', 'capozid', 'day', '(hypertension)', 'omeprazol', 'otc', 'loratadin', 'day,', 'multivitamin', 'day,', 'homemad', 'nopal', 'water', 'past', 'apriso,', 'colazal,', 'asacol', 'hd,', 'prednisone,', 'azathioprin', 'discontinu', 'due', 'elev', 'liver', 'enzym'], tags=['text_1']),\n",
       " LabeledSentence(words=['tuesday', 'pm', 'et', 'fda', 'approv', 'genentech', 'ocrevu', 'tm', '(ocrelizumab)', 'peopl', 'relaps', 'primari', 'progress', 'form', 'multipl', 'sclerosi', '(ms)', 'first', 'approv', 'treatment', 'primari', 'progress', 'ms', '(ppms)', ',', 'one', 'disabl', 'form', 'ms', 'ocrevu', 'import', 'new', 'treatment', 'option', 'peopl', 'relaps', 'form', 'multipl', 'sclerosi', '(rms)', 'demonstr', 'superior', 'efficaci', 'three', 'major', 'marker', 'diseas', 'activ', 'compar', 'rebif', 'key', 'find', 'two', 'phase', 'iii', 'rm', 'one', 'phase', 'iii', 'ppm', 'studi', 'includ', 'addit', 'inform', 'press', 'releas', 'reminder,', 'result', 'three', 'phase', 'iii', 'studi', 'recent', 'publish', 'januari', ',', 'issu', 'new', 'england', 'journal', 'medicin', '(nejm)', 'larg', 'phase', 'iii', 'study,', 'ocrevu', 'significantli', 'slow', 'disabl', 'progress', 'reduc', 'sign', 'diseas', 'activ', 'brain', 'compar', 'placebo', 'peopl', 'ppm', 'two', 'larg', 'phase', 'iii', 'clinic', 'studi', 'show', 'compar', 'interferon', 'beta', '(rebif', ')', 'peopl', 'rms,', 'ocrevu', 'reduc', 'relaps', 'nearli', 'half,', 'better', 'slow', 'disabl', 'progress', 'suppress', 'sign', 'diseas', 'activ', 'brain', 'two', 'year', 'ocrevu', 'repres', 'differ', 'scientif', 'approach', 'treat', 'ms', 'first', 'class', 'medicin', 'target', 'cd', 'posit', 'b', 'cells,', 'type', 'immun', 'cell', 'play', 'central', 'role', 'diseas', 'fda', 'approv', 'genentech', 'ocrevu', '(ocrelizumab)', 'relaps', 'primari', 'progress', 'form', 'multipl', 'sclerosi', 'first', 'approv', 'diseas', 'modifi', 'therapi', 'primari', 'progress', 'form', 'multipl', 'sclerosi', '(ppms)', 'one', 'disabl', 'form', 'multipl', 'sclerosi', '(ms)', 'import', 'new', 'treatment', 'option', 'peopl', 'relaps', 'form', 'multipl', 'sclerosi', '(rms)', 'demonstr', 'superior', 'efficaci', 'three', 'major', 'marker', 'diseas', 'activ', 'compar', 'rebif', 'favor', 'benefit', 'risk', 'profil', 'demonstr', 'three', 'larg', 'phase', 'iii', 'studi', 'divers', 'patient', 'population,', 'includ', 'earli', 'diseas', 'south', 'san', 'francisco,', 'ca', 'march', ',', 'genentech,', 'member', 'roch', 'group', '(six', 'ro,', 'rog', 'otcqx', 'rhhby),', 'announc', 'today', 'u', 'food', 'drug', 'administr', '(fda)', 'approv', 'ocrevu', '(ocrelizumab)', 'first', 'medicin', 'relaps', 'primari', 'progress', 'form', 'multipl', 'sclerosi', 'major', 'peopl', 'ms', 'relaps', 'form', 'primari', 'progress', 'ms', 'diagnosi', 'fda', 'approv', 'ocrevu', 'begin', 'new', 'era', 'ms', 'commun', 'repres', 'signific', 'scientif', 'advanc', 'first', 'class', 'b', 'cell', 'target', 'therapy,', 'said', 'sandra', 'horning,', ',', 'chief', 'medic', 'offic', 'head', 'global', 'product', 'develop', 'now,', 'fda', 'approv', 'treatment', 'avail', 'primari', 'progress', 'ms', 'community,', 'peopl', 'relaps', 'form', 'ms', 'continu', 'experi', 'diseas', 'activ', 'disabl', 'progress', 'despit', 'avail', 'therapi', 'believ', 'ocrevus,', 'given', 'everi', 'six', 'months,', 'potenti', 'chang', 'diseas', 'cours', 'peopl', 'ms,', 'commit', 'help', 'benefit', 'gain', 'access', 'medicin', 'two', 'ident', 'rm', 'phase', 'iii', 'studi', '(opera', 'opera', 'ii),', 'ocrevu', 'demonstr', 'superior', 'efficaci', 'three', 'major', 'marker', 'diseas', 'activ', 'reduc', 'relaps', 'per', 'year', 'nearli', 'half,', 'slow', 'worsen', 'disabl', 'significantli', 'reduc', 'mri', 'lesion', 'compar', 'rebif', '(high', 'dose', 'interferon', 'beta', 'a)', 'two', 'year', 'control', 'treatment', 'period', 'similar', 'proport', 'peopl', 'ocrevu', 'group', 'experienc', 'low', 'rate', 'seriou', 'advers', 'event', 'seriou', 'infect', 'compar', 'peopl', 'high', 'dose', 'interferon', 'beta', 'group', 'rm', 'studi', 'separ', 'ppm', 'phase', 'iii', 'studi', '(oratorio),', 'ocrevu', 'first', 'treatment', 'significantli', 'slow', 'disabl', 'progress', 'reduc', 'sign', 'diseas', 'activ', 'brain', '(mri', 'lesions)', 'compar', 'placebo', 'median', 'follow', 'three', 'year', 'similar', 'proport', 'peopl', 'ocrevu', 'group', 'experienc', 'advers', 'event', 'low', 'rate', 'seriou', 'advers', 'event', 'compar', 'peopl', 'placebo', 'group', 'ppm', 'studi', 'common', 'side', 'effect', 'associ', 'ocrevu', 'phase', 'iii', 'studi', 'includ', 'infus', 'reaction', 'upper', 'respiratori', 'tract', 'infections,', 'mostli', 'mild', 'moder', 'sever', 'result', 'three', 'phase', 'iii', 'studi', 'recent', 'publish', 'januari', ',', 'issu', 'new', 'england', 'journal', 'medicin', '(nejm)', 'excit', 'day', 'everyon', 'touch', 'ms,', 'diseas', 'strike', 'prime', 'person', 'life', 'may', 'start', 'career', 'family,', 'said', 'june', 'halper,', 'msn,', 'apn', 'c,', 'mscn,', 'faan,', 'chief', 'execut', 'offic', 'consortium', 'ms', 'center', 'eagerli', 'await', 'fda', 'approv', 'ocrevu', 'offer', 'new,', 'highli', 'efficaci', 'treatment', 'option', 'peopl', 'relaps', 'multipl', 'sclerosis,', 'also', 'first', 'diseas', 'modifi', 'therapi', 'indic', 'primari', 'progress', 'multipl', 'sclerosis,', 'highli', 'disabl', 'type', 'chronic', 'diseas', 'mani', 'peopl', 'live', 'ms,', 'fda', 'approv', 'sourc', 'hope', 'ocrevu', 'avail', 'peopl', 'u', 'within', 'two', 'week', 'genentech', 'commit', 'help', 'peopl', 'access', 'medicin', 'prescrib', 'offer', 'comprehens', 'servic', 'peopl', 'prescrib', 'ocrevu', 'help', 'minim', 'barrier', 'access', 'reimburs', 'patient', 'call', 'ocrevu', 'inform', 'peopl', 'qualify,', 'genentech', 'plan', 'offer', 'patient', 'assist', 'program', 'genentech', 'access', 'solut', 'inform', 'also', 'avail', '()', 'access', '()', 'ocrevu', 'market', 'author', 'applic', '(maa)', 'also', 'valid', 'european', 'medicin', 'agenc', '(ema)', 'current', 'review', 'opera', 'opera', 'ii', 'studi', 'relaps', 'form', 'ms', 'opera', 'opera', 'ii', 'phase', 'iii,', 'randomized,', 'doubl', 'blind,', 'doubl', 'dummy,', 'global', 'multi', 'center', 'studi', 'evalu', 'efficaci', 'safeti', 'ocrevu', '(', 'mg', 'administ', 'intraven', 'infus', 'everi', 'six', 'months)', 'compar', 'interferon', 'beta', '(', 'mcg', 'administ', 'subcutan', 'inject', 'three', 'time', 'per', 'week)', ',', 'peopl', 'relaps', 'form', 'ms', 'studies,', 'relaps', 'ms', '(rms)', 'defin', 'relaps', 'remit', 'ms', '(rrms)', 'secondari', 'progress', 'ms', '(spms)', 'relaps', 'oratorio', 'studi', 'primari', 'progress', 'ms', 'oratorio', 'phase', 'iii,', 'randomized,', 'doubl', 'blind,', 'global', 'multi', 'center', 'studi', 'evalu', 'efficaci', 'safeti', 'ocrevu', '(', 'mg', 'administ', 'intraven', 'infus', 'everi', 'six', 'month', 'given', 'two', 'mg', 'infus', 'two', 'week', 'apart)', 'compar', 'placebo', 'peopl', 'ppm', 'blind', 'treatment', 'period', 'oratorio', 'studi', 'continu', 'patient', 'receiv', 'least', 'week', 'either', 'ocrevu', 'placebo', 'predefin', 'number', 'confirm', 'disabl', 'progress', '(cdp)', 'event', 'reach', 'overal', 'studi', 'summari', 'data', 'opera', 'i,', 'opera', 'ii', 'oratorio', 'studi', 'support', 'approv', 'key', 'data', 'rm', 'patient', 'treat', 'ocrevu', 'show', 'percent', 'percent', 'rel', 'reduct', 'annual', 'relaps', 'rate', '(arr)', 'compar', 'interferon', 'beta', 'two', 'year', 'period', 'opera', 'opera', 'ii,', 'respect', '(p', 'p', ')', 'percent', 'rel', 'risk', 'reduct', 'confirm', 'disabl', 'progress', '(cdp)', 'sustain', 'week', 'compar', 'interferon', 'beta', 'pool', 'analysi', 'opera', 'opera', 'ii,', 'measur', 'expand', 'disabl', 'statu', 'scale', '(edss)', '(p', ')', 'percent', 'percent', 'rel', 'reduct', 'total', 'number', 'gadolinium', 'enhanc', 'lesion', 'compar', 'interferon', 'beta', 'opera', 'opera', 'ii,', 'respect', '(p', 'p', ')', 'percent', 'percent', 'rel', 'reduct', 'total', 'number', 'new', 'enlarg', 'lesion', 'compar', 'interferon', 'beta', 'opera', 'opera', 'ii,', 'respect', '(p', 'p', ')', 'key', 'data', 'ppm', 'patient', 'treat', 'ocrevu', 'show', 'percent', 'rel', 'risk', 'reduct', 'cdp', 'sustain', 'least', 'week', 'compar', 'placebo,', 'measur', 'edss', '(p', ')', 'cm', 'mean', 'chang', 'volum', 'brain', 'hyperintens', 'lesion', 'compar', 'cm', 'mean', 'chang', 'volum', 'placebo', 'treat', 'patient', 'week', '(p', ')', 'percent', 'rel', 'risk', 'reduct', 'proport', 'patient', 'percent', 'worsen', 'time', 'foot', 'walk', 'confirm', 'week', 'common', 'side', 'effect', 'associ', 'ocrevu', 'phase', 'iii', 'studi', 'infus', 'reaction', 'upper', 'respiratori', 'tract', 'infections,', 'mostli', 'mild', 'moder', 'sever', 'potenti', 'seriou', 'side', 'effect', 'may', 'includ', 'infus', 'reactions,', 'infect', 'malign', 'routin', 'screen', 'requir', 'base', 'age', 'medic', 'histori', 'multipl', 'sclerosi', 'multipl', 'sclerosi', '(ms)', 'chronic', 'diseas', 'affect', 'estim', ',', 'peopl', 'u', ',', 'current', 'cure', 'ms', 'occur', 'immun', 'system', 'abnorm', 'attack', 'insul', 'support', 'around', 'nerv', 'cell', '(myelin', 'sheath)', 'brain,', 'spinal', 'cord', 'optic', 'nerves,', 'caus', 'inflamm', 'consequ', 'damag', 'damag', 'caus', 'wide', 'rang', 'symptoms,', 'includ', 'muscl', 'weakness,', 'fatigu', 'difficulti', 'seeing,', 'may', 'eventu', 'lead', 'disabl', 'peopl', 'ms', 'experi', 'first', 'symptom', 'year', 'age,', 'make', 'diseas', 'lead', 'caus', 'non', 'traumat', 'disabl', 'younger', 'adult', 'relaps', 'remit', 'ms', '(rrms)', 'common', 'form', 'diseas', 'character', 'episod', 'new', 'worsen', 'sign', 'symptom', '(relapses)', 'follow', 'period', 'recoveri', 'approxim', 'percent', 'peopl', 'ms', 'initi', 'diagnos', 'rrm', 'major', 'peopl', 'diagnos', 'rrm', 'eventu', 'transit', 'secondari', 'progress', 'ms', '(spms),', 'experi', 'steadili', 'worsen', 'disabl', 'time', 'relaps', 'form', 'ms', '(rms)', 'includ', 'peopl', 'rrm', 'peopl', 'spm', 'continu', 'experi', 'relaps', 'primari', 'progress', 'ms', '(ppms)', 'debilit', 'form', 'diseas', 'mark', 'steadili', 'worsen', 'symptom', 'typic', 'without', 'distinct', 'relaps', 'period', 'remiss', 'approxim', 'percent', 'peopl', 'ms', 'diagnos', 'primari', 'progress', 'form', 'diseas', 'now,', 'fda', 'approv', 'treatment', 'ppm', 'peopl', 'form', 'ms', 'experi', 'diseas', 'activ', 'inflamm', 'nervou', 'system', 'perman', 'loss', 'nerv', 'cell', 'brain', 'even', 'clinic', 'symptom', 'appar', 'appear', 'get', 'wors', 'import', 'goal', 'treat', 'ms', 'reduc', 'diseas', 'activ', 'soon', 'possibl', 'slow', 'quickli', 'person', 'disabl', 'progress', 'despit', 'avail', 'diseas', 'modifi', 'treatment', '(dmts),', 'peopl', 'rm', 'continu', 'experi', 'diseas', 'activ', 'disabl', 'progress', 'ocrevu', '(ocrelizumab)', 'ocrevu', 'human', 'monoclon', 'antibodi', 'design', 'select', 'target', 'cd', 'posit', 'b', 'cells,', 'specif', 'type', 'immun', 'cell', 'thought', 'key', 'contributor', 'myelin', '(nerv', 'cell', 'insul', 'support)', 'axon', '(nerv', 'cell)', 'damag', 'nerv', 'cell', 'damag', 'lead', 'disabl', 'peopl', 'ms', 'base', 'preclin', 'studies,', 'ocrevu', 'bind', 'cd', 'cell', 'surfac', 'protein', 'express', 'certain', 'b', 'cells,', 'stem', 'cell', 'plasma', 'cells,', 'therefor', 'import', 'function', 'immun', 'system', 'may', 'preserv', 'ocrevu', 'administ', 'intraven', 'infus', 'everi', 'six', 'month', 'first', 'dose', 'given', 'two', 'mg', 'infus', 'given', 'two', 'week', 'apart', 'subsequ', 'dose', 'given', 'singl', 'mg', 'infus', 'ocrevu', 'u', 'indic', 'ocrevu', 'prescript', 'medicin', 'use', 'treat', 'adult', 'relaps', 'primari', 'progress', 'form', 'multipl', 'sclerosi', 'known', 'ocrevu', 'safe', 'effect', 'children', 'import', 'safeti', 'inform', 'receiv', 'ocrevus?', 'receiv', 'ocrevu', 'patient', 'activ', 'hepat', 'b', 'viru', '(hbv)', 'infect', 'receiv', 'ocrevu', 'patient', 'life', 'threaten', 'allerg', 'reaction', 'ocrevu', 'patient', 'tell', 'healthcar', 'provid', 'allerg', 'reaction', 'ocrevu', 'ingredi', 'past', 'import', 'inform', 'ocrevus?', 'ocrevu', 'caus', 'seriou', 'side', 'effects,', 'includ', 'infus', 'reaction', 'ocrevu', 'caus', 'infus', 'reaction', 'seriou', 'requir', 'patient', 'hospit', 'patient', 'monitor', 'infus', 'least', 'hour', 'infus', 'ocrevu', 'sign', 'symptom', 'infus', 'reaction', 'patient', 'tell', 'healthcar', 'provid', 'nurs', 'get', 'symptom', 'itchi', 'skin,', 'rash,', 'hives,', 'tiredness,', 'cough', 'wheezing,', 'troubl', 'breathing,', 'throat', 'irrit', 'pain,', 'feel', 'faint,', 'fever,', 'red', 'face', '(flushing),', 'nausea,', 'headache,', 'swell', 'throat,', 'dizziness,', 'short', 'breath,', 'fatigue,', 'fast', 'heart', 'beat', 'infus', 'reaction', 'happen', 'hour', 'infus', 'import', 'patient', 'call', 'healthcar', 'provid', 'right', 'away', 'get', 'sign', 'symptom', 'list', 'infus', 'patient', 'get', 'infus', 'reactions,', 'healthcar', 'provid', 'may', 'need', 'stop', 'slow', 'rate', 'infus', 'infect', 'ocrevu', 'increas', 'patient', 'risk', 'get', 'upper', 'respiratori', 'tract', 'infections,', 'lower', 'respiratori', 'tract', 'infections,', 'skin', 'infections,', 'herp', 'infect', 'patient', 'tell', 'healthcar', 'provid', 'infect', 'follow', 'sign', 'infect', 'includ', 'fever,', 'chills,', 'cough', 'go', 'away,', 'sign', 'herp', '(such', 'cold', 'sores,', 'shingles,', 'genit', 'sores)', 'sign', 'happen', 'treatment', 'patient', 'receiv', 'last', 'dose', 'ocrevu', 'patient', 'activ', 'infection,', 'healthcar', 'provid', 'delay', 'treatment', 'ocrevu', 'infect', 'gone', 'progress', 'multifoc', 'leukoencephalopathi', '(pml)', 'although', 'case', 'seen', 'ocrevu', 'treatment,', 'pml', 'may', 'happen', 'ocrevu', 'pml', 'rare', 'brain', 'infect', 'usual', 'lead', 'death', 'sever', 'disabl', 'patient', 'tell', 'healthcar', 'provid', 'right', 'away', 'new', 'worsen', 'neurolog', 'sign', 'symptom', 'may', 'includ', 'problem', 'thinking,', 'balance,', 'eyesight,', 'weak', 'one', 'side', 'body,', 'strength,', 'use', 'arm', 'leg', 'hepat', 'b', 'viru', '(hbv)', 'reactiv', 'start', 'treatment', 'ocrevus,', 'patient', 'healthcar', 'provid', 'blood', 'test', 'check', 'hepat', 'b', 'viral', 'infect', 'patient', 'ever', 'hepat', 'b', 'viru', 'infection,', 'hepat', 'b', 'viru', 'may', 'becom', 'activ', 'treatment', 'ocrevu', 'hepat', 'b', 'viru', 'becom', 'activ', '(call', 'reactivation)', 'may', 'caus', 'seriou', 'liver', 'problem', 'includ', 'liver', 'failur', 'death', 'healthcar', 'provid', 'monitor', 'patient', 'risk', 'hepat', 'b', 'viru', 'reactiv', 'treatment', 'stop', 'receiv', 'ocrevu', 'weaken', 'immun', 'system', 'ocrevu', 'taken', 'medicin', 'weaken', 'immun', 'system', 'could', 'increas', 'patient', 'risk', 'get', 'infect', 'receiv', 'ocrevus,', 'patient', 'tell', 'healthcar', 'provid', 'medic', 'conditions,', 'includ', 'ever', 'taken,', 'take,', 'plan', 'take', 'medicin', 'affect', 'immun', 'system,', 'treatment', 'ms', 'ever', 'hepat', 'b', 'carrier', 'hepat', 'b', 'viru', 'recent', 'vaccin', 'schedul', 'receiv', 'vaccin', 'patient', 'receiv', 'requir', 'vaccin', 'least', 'week', 'start', 'treatment', 'ocrevu', 'patient', 'receiv', 'certain', 'vaccin', '(call', 'live', 'live', 'attenu', 'vaccines)', 'treat', 'ocrevu', 'healthcar', 'provid', 'tell', 'immun', 'system', 'longer', 'weaken', 'pregnant,', 'think', 'might', 'pregnant,', 'plan', 'becom', 'pregnant', 'known', 'ocrevu', 'harm', 'unborn', 'babi', 'patient', 'use', 'birth', 'control', '(contraception)', 'treatment', 'ocrevu', 'month', 'last', 'infus', 'ocrevu', 'breastfeed', 'plan', 'breastfe', 'known', 'ocrevu', 'pass', 'breast', 'milk', 'patient', 'talk', 'healthcar', 'provid', 'best', 'way', 'feed', 'babi', 'patient', 'take', 'ocrevu', 'possibl', 'side', 'effect', 'ocrevus?', 'ocrevu', 'may', 'caus', 'seriou', 'side', 'effects,', 'includ', 'risk', 'cancer', '(malignancies)', 'includ', 'breast', 'cancer', 'patient', 'follow', 'healthcar', 'provid', 'recommend', 'standard', 'screen', 'guidelin', 'breast', 'cancer', 'common', 'side', 'effect', 'includ', 'infus', 'reaction', 'infect', 'possibl', 'side', 'effect', 'ocrevu', 'patient', 'call', 'doctor', 'medic', 'advic', 'side', 'effect', 'side', 'effect', 'may', 'also', 'report', 'fda', 'fda', 'information,', 'go', 'call', 'addit', 'safeti', 'information,', 'pleas', 'see', 'full', 'prescrib', 'inform', 'medic', 'guid', 'genentech', 'neurosci', 'neurosci', 'major', 'focu', 'research', 'develop', 'genentech', 'roch', 'compani', 'goal', 'develop', 'treatment', 'option', 'base', 'biolog', 'nervou', 'system', 'help', 'improv', 'live', 'peopl', 'chronic', 'potenti', 'devast', 'diseas', 'roch', 'dozen', 'investig', 'medicin', 'clinic', 'develop', 'diseas', 'includ', 'multipl', 'sclerosis,', 'alzheim', 'disease,', 'spinal', 'muscular', 'atrophy,', 'parkinson', 'diseas', 'autism', 'genentech', 'found', 'year', 'ago,', 'genentech', 'lead', 'biotechnolog', 'compani', 'discovers,', 'develops,', 'manufactur', 'commerci', 'medicin', 'treat', 'patient', 'seriou', 'life', 'threaten', 'medic', 'condit', 'company,', 'member', 'roch', 'group,', 'headquart', 'south', 'san', 'francisco,', 'california', 'addit', 'inform', 'company,', 'pleas', 'visit', 'trademark', 'use', 'mention', 'releas', 'protect', 'law', 'rebif', 'regist', 'trademark', 'merck', 'kgaa', 'emd', 'serono,', 'inc'], tags=['text_2']),\n",
       " LabeledSentence(words=['hi', 'mohit,', 'welcom', 'grace', 'sorri', 'hear', 'recent', 'progress', 'father', 'lung', 'cancer', 'though', 'oncologist', 'like', 'continu', 'tarceva', 'even', 'progression,', 'combin', 'drug', 'alimta', '(pemetrexed),', 'often', 'progress', 'signific', 'enough', 'third', 'gener', 'egfr', 'tki', 'tagrisso', 'chosen', 'addition,', 'doctor', 'choos', 'combin', 'afatinib', 'cetuximab,', 'effect', 'quit', 'challeng', 'term', 'combin', 'side', 'effect', 'egfr', 'posit', 'patient', 'often', 'overlook', 'standard', 'chemotherapy,', 'tend', 'quit', 'effect', 'subgroup', 'patient', 'regimen', 'carboplatin', 'cisplatin,', 'combin', 'alimta', 'taxol', 'often', 'use', 'jimc', 'forum', 'moder', 'jul', 'wife', 'liz', '(', 'never', 'smoker)', 'dx', 'stage', 'iv', 'nsclc', 'egfr', 'exon', 'cycl', 'carbo', 'alimta,', 'shrinkag', 'tarceva', 'mainten', 'mar', 'progression,', 'ad', 'alimta,', 'stabl', 'sep', 'multipl', 'brain', 'mets,', 'wbr', 'oct', 'larg', 'pericardi', 'effusion,', 'tamponad', 'jan', 'progression,', 'start', 'abraxan', 'jun', 'new', 'liver,', 'brain', 'mets,', 'add', 'tarceva', 'oct', 'dx', 'leptomening', 'carcinomatosi', 'puls', 'tarceva', 'rest', 'nov', 'sinc'], tags=['text_3']),\n",
       " LabeledSentence(words=['comment', 'diagnos', 'wet', 'armd', 'year', 'ago', 'switch', 'eylea', 'sever', 'month', 'ago', 'initi', 'profound', 'impact', 'reduc', 'wet', 'leakag', 'entir', 'leakag', 'return', 'bi', 'monthli', 'inject', 'skip', 'next', 'inject', 'restor', 'impact', 'entir', 'elimin', 'leakag', 'hope', 'definit', 'result', 'next', 'inject', 'inject', 'test', 'inconveni', 'acco', 'show', 'full', 'comment', 'comment', 'diagnos', 'wet', 'armd', 'year', 'ago', 'switch', 'eylea', 'sever', 'month', 'ago', 'initi', 'profound', 'impact', 'reduc', 'wet', 'leakag', 'entir', 'leakag', 'return', 'bi', 'monthli', 'inject', 'skip', 'next', 'inject', 'restor', 'impact', 'entir', 'elimin', 'leakag', 'hope', 'definit', 'result', 'next', 'inject', 'inject', 'test', 'inconveni', 'accompani', 'minor', 'ach', 'hide', 'full', 'comment'], tags=['text_4']),\n",
       " LabeledSentence(words=['patient', 'full', 'genom', 'test', 'diagnosi', 'learn', 'kra', 'driver', 'mutation,', 'benefit,', 'patient,', 'full', 'genom', 'test', 'progression?', '(especially,', 'given', 'cost', 'tests)', 'never', 'heard', 'kra', 'mutat', 'chang', 'thank', 'you,', 'cathi', 'dx', 'stage', 'iv', 'adeno,', 'dec', 'kra', 'gd,', 'pdl', 'tumor', 'sampl', 'st', 'line', 'carboplatin', 'alimta', 'alimta', 'mainten', '(work', 'well)', 'nd', 'line', 'keytruda', '(didn', 'work)', 'rd', 'line', 'imrt', 'lung', 'radiation,', 'gy', 'current', 'watch', 'wait'], tags=['text_5'])]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_texts[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 5215/5215 [00:00<00:00, 868987.94it/s]\n"
     ]
    }
   ],
   "source": [
    "#Now let’s train a doc2vec model.\n",
    "model_d2v = gensim.models.Doc2Vec(\n",
    "    dm=1, # dm = 1 for ‘distributed memory’ model                                  \n",
    "    dm_mean=1, # dm = 1 for using mean of the context word vectors                                 \n",
    "    vector_size=200, # no. of desired features                                  \n",
    "    window=5, # width of the context window                                  \n",
    "    negative=7, # if > 0 then negative sampling will be used                                 \n",
    "    min_count=5, # Ignores all words with total frequency lower than 2.                                  \n",
    "    workers=3, # no. of cores                                  \n",
    "    alpha=0.1, # learning rate                                  \n",
    "    seed = 23) \n",
    "model_d2v.build_vocab([i for i in tqdm(labeled_texts)])\n",
    "model_d2v.train(labeled_texts, total_examples= len(combi['tidy_text']), epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5215, 200)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preparing doc2vec Feature Set\n",
    "docvec_arrays = np.zeros((len(tokenized_text), 200)) \n",
    "for i in range(len(combi)):\n",
    "    docvec_arrays[i,:] = model_d2v.docvecs[i].reshape((1,200))    \n",
    "\n",
    "docvec_df = pd.DataFrame(docvec_arrays) \n",
    "print(docvec_df.shape)\n",
    "docvec_df['drug']=combi['drug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into  train and test features \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score , accuracy_score,confusion_matrix , classification_report\n",
    "\n",
    "# Bag-of-Words Features\n",
    "train_bow = bow.iloc[:2291,:].values\n",
    "test_bow = bow.iloc[2291:,:].values \n",
    "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train_under['sentiment'], random_state=2,test_size=0.2) #2\n",
    "\n",
    "#TF_IDF Bag-of-Words Features\n",
    "train_tfidf = tfidf.iloc[:2291,:].values\n",
    "test_tfidf = tfidf.iloc[2291:,:].values\n",
    "xtrain_tfidf = train_tfidf[ytrain.index] \n",
    "xvalid_tfidf = train_tfidf[yvalid.index]\n",
    "\n",
    "train_w2v = wordvec_df.iloc[:2291,:]\n",
    "test_w2v = wordvec_df.iloc[2291:,:] \n",
    "xtrain_w2v = train_w2v.iloc[ytrain.index,:] \n",
    "xvalid_w2v = train_w2v.iloc[yvalid.index,:]\n",
    "\n",
    "#Doc2Vec Features\n",
    "train_d2v = docvec_df.iloc[:2291,:] \n",
    "test_d2v = docvec_df.iloc[2291:,:] \n",
    "xtrain_d2v = train_d2v.iloc[ytrain.index,:] \n",
    "xvalid_d2v = train_d2v.iloc[yvalid.index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Bag-of-Words Features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_bow, ytrain) \n",
    "prediction = rf.predict(xvalid_bow) \n",
    "#print(f1_score(yvalid, prediction) ) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))  \n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf1 = RandomForestClassifier(n_estimators=400, random_state=11).fit(train_bow,train_under['sentiment'])\n",
    "test_pred = rf1.predict(test_bow) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('rf_bow_under.csv', index=False)\n",
    "#Public Leaderboard F1 Score: 0.448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Features\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_tfidf, ytrain) \n",
    "prediction = rf.predict(xvalid_tfidf) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))  \n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n",
    "\n",
    "#Public Leaderboard F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=400, random_state=11).fit(train_tfidf,train_under['sentiment'])\n",
    "test_pred = rf1.predict(test_tfidf) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('rf_tfidf_under.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7395833333333334"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word 2 vec\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_w2v, ytrain) \n",
    "prediction = rf.predict(xvalid_w2v) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))   \n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n",
    "\n",
    "#Public Leaderboard F1 Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7357954545454546"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#doc 2 vec\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, random_state=11).fit(xtrain_d2v, ytrain) \n",
    "prediction = rf.predict(xvalid_d2v) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  #0.7380050505050505\n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "\n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n",
    "#Public Leaderboard F1 Score: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.49673202614379086\n",
      "f1score 0.49484416372189405\n",
      "accuracy [[52 37 41]\n",
      " [26 93 45]\n",
      " [36 46 83]]\n",
      "f1score               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.40      0.43       130\n",
      "           1       0.53      0.57      0.55       164\n",
      "           2       0.49      0.50      0.50       165\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       459\n",
      "   macro avg       0.49      0.49      0.49       459\n",
      "weighted avg       0.49      0.50      0.49       459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Bag-of-Words Features\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_bow, ytrain) \n",
    "prediction = xgb_model.predict(xvalid_bow) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(train_bow, train1['sentiment']) \n",
    "test_pred = xgb_model.predict(test_bow) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('xgb_bowundersampling.csv', index=False)\n",
    "#Public Leaderboard F1 Score: 0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.49673202614379086\n",
      "f1score 0.49484416372189405\n",
      "[[52 37 41]\n",
      " [26 93 45]\n",
      " [36 46 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.40      0.43       130\n",
      "           1       0.53      0.57      0.55       164\n",
      "           2       0.49      0.50      0.50       165\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       459\n",
      "   macro avg       0.49      0.49      0.49       459\n",
      "weighted avg       0.49      0.50      0.49       459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF Features\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000).fit(xtrain_tfidf, ytrain) \n",
    "prediction = xgb.predict(xvalid_tfidf) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n",
    "\n",
    "#Public Leaderboard F1 Score: 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(train_tfidf, train_under['sentiment']) \n",
    "test_pred = xgb_model.predict(test_tfidf) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('xgb_tfidf_under.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.49673202614379086\n",
      "f1score 0.49484416372189405\n",
      "[[52 37 41]\n",
      " [26 93 45]\n",
      " [36 46 83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.40      0.43       130\n",
      "           1       0.53      0.57      0.55       164\n",
      "           2       0.49      0.50      0.50       165\n",
      "\n",
      "   micro avg       0.50      0.50      0.50       459\n",
      "   macro avg       0.49      0.49      0.49       459\n",
      "weighted avg       0.49      0.50      0.49       459\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Word 2 Vec\n",
    "\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_w2v, ytrain) \n",
    "prediction = xgb.predict(xvalid_w2v) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(train_w2v, train_under['sentiment']) \n",
    "test_pred = xgb_model.predict(test_w2v) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('xgb_w2v_under.csv', index=False)\n",
    "#Public Leaderboard  Score: 0.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7310606060606061\n",
      "f1score 0.6376817215819405\n"
     ]
    }
   ],
   "source": [
    "#Doc2Vec Features\n",
    "xgb = XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3).fit(xtrain_d2v, ytrain) \n",
    "prediction = xgb.predict(xvalid_d2v) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted')) \n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(max_depth=6, n_estimators=1000).fit(train_d2v, train_under['sentiment']) \n",
    "test_pred = xgb_model.predict(test_d2v) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('xgb_d2v_under.csv', index=False)\n",
    "#Public Leaderboard  Score: 0.37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(C=30, class_weight='balanced', solver='sag', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40, \n",
    "                         verbose=1, max_iter = 1000)\n",
    "clf.fit(xtrain_bow, ytrain)\n",
    "prediction = clf.predict(xvalid_bow) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 101 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.7min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=30, class_weight='balanced', solver='sag', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40, \n",
    "                         verbose=1, max_iter = 1000).fit(train_bow, train_under['sentiment']) \n",
    "test_pred = clf.predict(test_bow) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('log_bow_under.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec\n",
    "clf = LogisticRegression(C=30, class_weight='balanced', solver='sag', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40, \n",
    "                         verbose=1, max_iter = 1000)\n",
    "clf.fit(xtrain_w2v, ytrain)\n",
    "prediction = clf.predict(xvalid_w2v) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))\n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "print(confusion_matrix(yvalid, prediction))  \n",
    "print(classification_report(y_true=yvalid, y_pred=prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter reached after 21 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   21.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=30, class_weight='balanced', solver='sag', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40, \n",
    "                         verbose=1, max_iter = 1000).fit(train_w2v, train_under['sentiment']) \n",
    "test_pred = clf.predict(test_w2v) \n",
    "test['sentiment'] = test_pred \n",
    "submission = test[['unique_hash','sentiment']] \n",
    "submission.to_csv('log_w2v_under.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5284090909090909\n",
      "f1score 0.5717391090856773\n",
      "accuracy [[ 64  23  39]\n",
      " [ 38  73  42]\n",
      " [185 171 421]]\n",
      "f1score               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.51      0.31       126\n",
      "           1       0.27      0.48      0.35       153\n",
      "           2       0.84      0.54      0.66       777\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1056\n",
      "   macro avg       0.45      0.51      0.44      1056\n",
      "weighted avg       0.68      0.53      0.57      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb=GaussianNB()\n",
    "nb.fit(xtrain_bow, ytrain)\n",
    "prediction = clf.predict(xvalid_bow) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "print('accuracy',confusion_matrix(yvalid, prediction))  \n",
    "print('f1score',classification_report(y_true=yvalid, y_pred=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5284090909090909\n",
      "f1score 0.5717391090856773\n",
      "accuracy [[ 64  23  39]\n",
      " [ 38  73  42]\n",
      " [185 171 421]]\n",
      "f1score               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.51      0.31       126\n",
      "           1       0.27      0.48      0.35       153\n",
      "           2       0.84      0.54      0.66       777\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1056\n",
      "   macro avg       0.45      0.51      0.44      1056\n",
      "weighted avg       0.68      0.53      0.57      1056\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sbg=SGDClassifier(loss='modified_huber',shuffle=True,random_state=101).fit(xtrain_bow, ytrain)\n",
    "prediction = clf.predict(xvalid_bow) \n",
    "print('accuracy',accuracy_score(yvalid, prediction))  \n",
    "print('f1score',f1_score(y_true=yvalid, y_pred=prediction, average='weighted'))\n",
    "print('accuracy',confusion_matrix(yvalid, prediction))  \n",
    "print('f1score',classification_report(y_true=yvalid, y_pred=prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Model is Word2Vec model used with under sampling XGBoost\n",
    "Public Leaderboard F1 Score: 0.4614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
